{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIM-C2-1920-20128822.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBYpHGoo_FI",
        "colab_type": "text"
      },
      "source": [
        "# Optimizing neurally diverse architectures for small and imbalanced subsets of MNIST  \n",
        "\n",
        "* This notebook is for exploring architectural specifications for neurally diverse architectures on a small and imbalanced version of MNIST.\n",
        "* The architecture includes pre-branch, branch, and post-branch segments. Neural diversity is implemented specifically at the level of activation function non-linearity and a small wet of \"connection functions\" (i.e. linear, identity, and element-wise scaling).\n",
        "* A highly simplistic form of architectural search based on stochastic hill-climbing with some differential search is included, for your reference.\n",
        "* 0.3.7 - added id and elementwise scaling weight functions.\n",
        "* 0.3.8 - added training acuracies; do_train is now returning the model with the best validation error.\n",
        "* 0.3.9 - added basic DropCircuit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYpp0HSto7FU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version = '_v0p3p9_'\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "\n",
        "from sklearn import cluster, datasets, mixture\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from itertools import cycle, islice\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "import copy\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "# ===============\n",
        "# Initializations\n",
        "# ===============\n",
        "\n",
        "!pip install torch torchvision\n",
        "\n",
        "# ===============\n",
        "# Parameters\n",
        "# ===============\n",
        "\n",
        "# --- General framework arguments\n",
        "\n",
        "args = {}\n",
        "kwargs = {}\n",
        "args['num_train_batch'] = 1  # number of MNIST training batches\n",
        "args['num_valid_batch'] = 10  # number of MNIST validation batches\n",
        "args['train_batch_size'] = 100   # training batch size\n",
        "args['valid_batch_size'] = 100   # validation batch size\n",
        "args['test_batch_size'] = 10\n",
        "# args['epochs'] = 100  # number of training epochs \n",
        "args['lr'] = 1 # 0.1  # 0.01 # learning rate # this is over-written by the solution\n",
        "args['momentum'] = 0.5 # SGD momentum (default: 0.5); momentum is a moving average of our gradients (helps keep a useful direction)\n",
        "args['clip_level'] = 0.5  # gradient clip threshold\n",
        "args['seed']= 1 #random seed\n",
        "args['log_interval_epoch'] = 1 # display training loss every log_int... epochs\n",
        "args['cuda'] = True \n",
        "args['patience'] = 1000  # stop train. if the valid. err. hasn't improved by this num. of epochs\n",
        "args['noise_in'] = -1   # 0.5  # 0.15  # amount of noise to add to the training data\n",
        "args['noise_out'] = -1   #  probability of changing an output label to some random label\n",
        "args['verbose_train'] = False   # print status of model training?\n",
        "args['verbose_meta'] = True #  print status of architecture optimization?\n",
        "args['min_inst_class'] = 5 # minimum number of instances per class in the training set\n",
        "args['batch_max_tries'] = 10 # max. num. of attempts in extracting data batches\n",
        "args['save_best_chrom'] = False # False  # save the best chromose in Google Drive?\n",
        "args['dc_prob_drop'] = 0.5 # 0.5 # probability of dropping a circuit (DropCircuit)\n",
        "args['prob_sel_branch'] = 0.5 # probability of architectural search selecting/using a branch (this is not DropCircuit) \n",
        "\n",
        "# --- Neural architectural limits\n",
        "\n",
        "args['num_epochs_search'] = 10 # 100  # number of epochs for training during architecture search \n",
        "args['num_epochs_test'] = 1000  # num. epochs for training during the final test\n",
        "limits = {}\n",
        "limits['min_layer_nodes'] =  10 # 5 # 50\n",
        "limits['max_layer_nodes'] = 100\n",
        "limits['max_pre_branch_layers'] = 5 # 3\n",
        "limits['max_branches'] = 10\n",
        "limits['max_branch_layers'] = 5  # 3\n",
        "limits['max_post_branch_layers'] = 5 # 3\n",
        "limits['lr1_min'] = 0.001\n",
        "limits['lr1_,max'] = 2\n",
        "limits['lr2_min'] = 0.001\n",
        "limits['lr2_,max'] = 2\n",
        "limits['moment_min'] = 0.001\n",
        "limits['moment_max'] = 1\n",
        "\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "num_train_instances = args['num_train_batch'] * args['train_batch_size']\n",
        "num_valid_instances = args['num_valid_batch'] * args['valid_batch_size']\n",
        "\n",
        "data_rand_seed = 1 # (other pre-tested seeds: 2, 3)\n",
        "\n",
        "# ============\n",
        "# Load dataset\n",
        "# ============\n",
        "\n",
        "# Seed creation\n",
        "torch.manual_seed(data_rand_seed)\n",
        "np.random.seed(data_rand_seed)\n",
        "\n",
        "a_data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "  \n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=a_data_transform),\n",
        "    batch_size=args['train_batch_size'], shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=a_data_transform),\n",
        "    batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n",
        "\n",
        "# Check whether we have enough instances per class\n",
        "# We want an imbalanced dataset but we don't want any specific label having\n",
        "# an \"insufficient\" number of instances.\n",
        "def check_enough_inst(batches, min_inst_per_class):\n",
        "  \n",
        "  # Concatenate training batch output labels\n",
        "  labels = batches[0][1].numpy()\n",
        "  for batch_i in range(1,args['num_train_batch']):\n",
        "    new_labels = batches[batch_i][1].numpy()\n",
        "    labels = np.concatenate((labels, new_labels))\n",
        "\n",
        "  labels = labels.tolist()\n",
        "\n",
        "  # Scan though labels\n",
        "  for a_label in range(10):  # assuming MNIST, of course\n",
        "    a_count = labels.count(a_label)\n",
        "    # If label count is < min_inst_per_class return False\n",
        "    if a_count < min_inst_per_class:\n",
        "      return False\n",
        "\n",
        "  # Return True\n",
        "  return True\n",
        "\n",
        "\n",
        "# Prepare data. Extract training and validation batches\n",
        "# This is where we make the problem \"small and imbalanced\"\n",
        "def extract_batches(a_loader, min_inst_per_class, max_tries):\n",
        "\n",
        "  # Keep trying until you have a required minimum number of instances \n",
        "  # per class in the training set (not elegant but ok for the range of \n",
        "  # \"min_inst_per_class\" we need)\n",
        "\n",
        "  for ti in range(max_tries):\n",
        "\n",
        "    print('Data extraction trial {}.'.format(ti))\n",
        "\n",
        "    # Initializations\n",
        "    train_batches = []\n",
        "    valid_batches = []\n",
        "    tot_batch_extract = args['num_train_batch'] + args['num_valid_batch']\n",
        "\n",
        "    # Extract\n",
        "    for batch_idx, (data, target) in enumerate(a_loader):\n",
        "\n",
        "      if batch_idx < args['num_train_batch']:\n",
        "        train_batches.append((data,target))\n",
        "      else:\n",
        "        valid_batches.append((data,target))\n",
        "        if batch_idx == tot_batch_extract - 1:\n",
        "          break\n",
        "\n",
        "    # Check minimum number of instances\n",
        "    enough_instaces = check_enough_inst(train_batches, min_inst_per_class)\n",
        "    if enough_instaces:\n",
        "      return train_batches, valid_batches\n",
        "\n",
        "  print('It was not possible to create a dataset.')\n",
        "  print('Consider increasing the overall number of instances, or')\n",
        "  print('decreasing the minimum instances per class allowed.')\n",
        "  return [], []\n",
        "\n",
        "\n",
        "train_batches, valid_batches = extract_batches(train_loader, args['min_inst_class'], args['batch_max_tries'])\n",
        "if args['verbose_train']:\n",
        "  print('Extracted {} train_batches, and {} valid_batches.'.format(len(train_batches), len(valid_batches)))\n",
        "\n",
        "# ===========================\n",
        "# Display histogram of labels\n",
        "# ===========================\n",
        "\n",
        "def disp_hist_labaels(batches):\n",
        "  # Concatenate training batch output labels\n",
        "  labels = batches[0][1].numpy()\n",
        "  for batch_i in range(1,args['num_train_batch']):\n",
        "    new_labels = batches[batch_i][1].numpy()\n",
        "    labels = np.concatenate((labels, new_labels))\n",
        "\n",
        "  num_bins = 10\n",
        "  n, bins, patches = plt.hist(labels, num_bins, facecolor='blue', alpha=0.5)\n",
        "  plt.show()\n",
        "\n",
        "disp_hist_labaels(train_batches)\n",
        "\n",
        "# ==========================\n",
        "# Display dataset\n",
        "# ==========================\n",
        "\n",
        "# This is not currently used\n",
        "\n",
        "# Display MNIST instances\n",
        "# Adapted from # https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb\n",
        "def disp_MNIST_inst(num_disp, X_train, y_train):\n",
        "  pltsize=1\n",
        "  plt.figure(figsize=(num_disp*pltsize, pltsize))\n",
        "  for i in range(num_disp):\n",
        "    plt.subplot(1,num_disp,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i,:,:].numpy().reshape(28,28), cmap=\"gray_r\")\n",
        "    plt.title('Class: '+str(y_train[i].item()))\n",
        "\n",
        "if args['verbose_train']:\n",
        "  X_train = train_batches[0][0]\n",
        "  y_train = train_batches[0][1]\n",
        "\n",
        "  disp_MNIST_inst(10, X_train, y_train)\n",
        "\n",
        "  sum_train_0 = X_train[0,:,:].sum()\n",
        "  min_train_0 = X_train[0,:,:].min()\n",
        "  max_train_0 = X_train[0,:,:].max()\n",
        "\n",
        "  print('X_train[0,:,:] --> sum ({}); min ({}); max ({}).'.format(sum_train_0, min_train_0, max_train_0))\n",
        "\n",
        "# ==========================\n",
        "# Design model\n",
        "# ==========================\n",
        "\n",
        "# \"Unseed\" the rest\n",
        "np.random.seed()\n",
        "a_rand_seed = np.random.randint(0,999999)\n",
        "torch.manual_seed(a_rand_seed)\n",
        "\n",
        "# Simple function to scale parameters\n",
        "# num is assume to be \\in [0,1]\n",
        "def scale_to_range(num,min_val,max_val):\n",
        "  range = max_val - min_val\n",
        "  return (num*range)+min_val\n",
        "\n",
        "# Simple layer for doing elementwise scaling\n",
        "# Adapated from https://stackoverflow.com/questions/51980654/pytorch-element-wise-filter-layer\n",
        "class Elem_Scaling_1D(nn.Module):\n",
        "  def __init__(self, num_nodes, bogus):  # clean-up \"bogus\"\n",
        "    super(Elem_Scaling_1D, self).__init__()\n",
        "    # Initialize\n",
        "    init_weights = np.random.normal(loc=0, scale=0.5, size=np.shape(num_nodes))\n",
        "    # Assign\n",
        "    self.weights = torch.from_numpy(init_weights)\n",
        "    #self.weights = nn.Parameter(torch.Tensor(1, num_nodes))  # trainable parameter\n",
        "\n",
        "  def forward(self, x):\n",
        "    # assuming x is of size num_inst-1-num_nodes\n",
        "    return x * self.weights  # element-wise multiplication\n",
        "\n",
        "# Class gradient-based neural diversity machine\n",
        "class GBNDM(nn.Module):   \n",
        "    def __init__(self, a_chromosome):   # assuming MNIST\n",
        "        super(GBNDM, self).__init__()\n",
        "\n",
        "        # --------- Pre-branch layers \n",
        "        self.chromosome = a_chromosome\n",
        "        self.pre_branch_layers = nn.ModuleList()\n",
        "        prev_num_out = 28*28\n",
        "        chrom_ind = 5 # skip over 3 learning rate param., 1 moment. p. (interp./used in training)\n",
        "        # and 1 exist-or-not (may use in future versions).\n",
        "        # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]\n",
        "        for i in range(limits['max_pre_branch_layers']):\n",
        "          \n",
        "          # Extract and interpret parameters\n",
        "          layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]\n",
        "          layer_params_real = self.interp_layer_param(layer_params_raw)\n",
        "        \n",
        "          # Decide on whether to create a layer or not\n",
        "          if i==0:  # the first layer of each segment is done by default\n",
        "            do_layer = True\n",
        "          else:\n",
        "            do_layer = layer_params_real[0]\n",
        "\n",
        "          if do_layer:\n",
        "            # Create layer    \n",
        "            a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)\n",
        "            # Append layer and update chromosome index\n",
        "            self.pre_branch_layers.append(a_layer)\n",
        "            \n",
        "          chrom_ind += 6\n",
        "\n",
        "        # --------- Branches\n",
        "        num_out_pre_branch = prev_num_out\n",
        "\n",
        "        self.branches = nn.ModuleList()\n",
        "\n",
        "        # Scan over branches\n",
        "        final_num_nodes = []  # keep track of the size of the final layer of each branch\n",
        "        count_branches = 0\n",
        "        for bi in range(limits['max_branches']):\n",
        "\n",
        "          # Initialize branch\n",
        "          branch_layers = nn.ModuleList()\n",
        "\n",
        "          # Do branch? Always do the first one by default\n",
        "          if (a_chromosome[chrom_ind] < args['prob_sel_branch']) or (bi == 0):\n",
        "            do_branch = True\n",
        "          else: \n",
        "            do_branch = False\n",
        "            \n",
        "          chrom_ind += 1\n",
        "\n",
        "          # Scan over branch layers\n",
        "\n",
        "          if do_branch:  # if doing branch\n",
        "\n",
        "            this_fin_num_nodes = 0\n",
        "\n",
        "            count_branches += 1\n",
        "            \n",
        "            for li in range(limits['max_branch_layers']):\n",
        "\n",
        "              # Extract and interpret parameters\n",
        "              layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]\n",
        "              layer_params_real = self.interp_layer_param(layer_params_raw)\n",
        "\n",
        "              # Decide on whether to create a layer or not\n",
        "              if li==0:  # the first layer of each branch is done by default\n",
        "                do_layer = True\n",
        "                prev_num_out = num_out_pre_branch\n",
        "              else:\n",
        "                do_layer = layer_params_real[0]\n",
        "\n",
        "              if do_layer:\n",
        "                # print('... temp ... this_fin_num_nodes: {}.'.format(this_fin_num_nodes))\n",
        "                # Create layer    \n",
        "                a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)\n",
        "                # Num_nodes - keep storing the latest one; the last latest is the final layer num_nodes\n",
        "                this_fin_num_nodes = prev_num_out\n",
        "                # Append layer and update chromosome index\n",
        "                branch_layers.append(a_layer)\n",
        "\n",
        "              chrom_ind += 6\n",
        "\n",
        "            # Append branch\n",
        "            final_num_nodes.append(this_fin_num_nodes)\n",
        "            #print('final_num_nodes: {}.'.format(final_num_nodes))\n",
        "            self.branches.append(branch_layers)\n",
        "\n",
        "          else: # if not doing branch you still need to increment chromosome index\n",
        "\n",
        "            chrom_ind += 6*limits['max_branch_layers']\n",
        "\n",
        "        self.num_branches = count_branches\n",
        "        self.dc_prob_activ = 1 - args['dc_prob_drop'] # probability of using a circuit\n",
        "        self.tot_nodes_branch_final = sum(final_num_nodes)\n",
        "        \n",
        "        # --------- Post-branch layers\n",
        "\n",
        "        prev_num_out = self.tot_nodes_branch_final\n",
        "        \n",
        "        # --- Create post-branch layers\n",
        "\n",
        "        chrom_ind += 1  # skip over the parameter pertaining to the existence or not of the post-branch segment\n",
        "\n",
        "        self.post_branch_layers = nn.ModuleList()\n",
        "        \n",
        "        # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]\n",
        "        for li in range(limits['max_post_branch_layers']):\n",
        "          \n",
        "          # Extract and interpret parameters\n",
        "          layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]\n",
        "          layer_params_real = self.interp_layer_param(layer_params_raw)\n",
        "        \n",
        "          # Decide on whether to create a layer or not\n",
        "          if li==0:  # the first layer of each segment is done by default\n",
        "            do_layer = True\n",
        "          else:\n",
        "            do_layer = layer_params_real[0]\n",
        "\n",
        "          if do_layer:\n",
        "            # Create layer    \n",
        "            a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)\n",
        "            # Append layer and update chromosome index\n",
        "            self.post_branch_layers.append(a_layer)\n",
        "            \n",
        "          chrom_ind += 6\n",
        "\n",
        "        # Create a final layer\n",
        "        self.final_layer = nn.Linear(prev_num_out, 10)\n",
        "\n",
        "    def forward(self, x, dc_mask = None):\n",
        "        \n",
        "        x = x.view(-1, 28*28)\n",
        "\n",
        "        # Apply pre-branch layers\n",
        "        for pi, a_layer in enumerate(self.pre_branch_layers):\n",
        "          #print('Pre-layer {}'.format(pi))\n",
        "          # if isinstance(a_layer, Elem_Scaling_1D):\n",
        "          #   set_trace()\n",
        "          x = a_layer(x)\n",
        "\n",
        "        # Apply branches\n",
        "        branch_finals = []\n",
        "        for bi, a_branch in enumerate(self.branches):\n",
        "          #print('Branch {}'.format(bi))\n",
        "          \n",
        "          z = x\n",
        "\n",
        "          for a_layer in a_branch:\n",
        "            # if isinstance(a_layer, Elem_Scaling_1D):\n",
        "            #   set_trace()\n",
        "            z = a_layer(z)\n",
        "          \n",
        "          if self.training:\n",
        "            z = (dc_mask[bi] * z) / self.dc_prob_activ  # if training apply DropCircuit\n",
        "          \n",
        "          branch_finals.append(z)\n",
        "\n",
        "        # Concatenate multi-branch final layers\n",
        "        x = torch.cat(branch_finals,dim=1)\n",
        "\n",
        "        # if not(self.training): # if not training then testing/evaluating\n",
        "        #   x = self.dc_prob_activ * x  # scaling due to DropCircuit\n",
        "      \n",
        "\n",
        "        # Apply post-branch layers\n",
        "        for pi, a_layer in enumerate(self.post_branch_layers):\n",
        "          #print('Post-layer: {}'.format(pi))\n",
        "          # if isinstance(a_layer, Elem_Scaling_1D):\n",
        "          #   set_trace()\n",
        "          x = a_layer(x)\n",
        "\n",
        "        # Apply final layer\n",
        "        x = self.final_layer(x)\n",
        "               \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "    # Method for interpreting layer parameters\n",
        "    # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]\n",
        "    def interp_layer_param(self, layer_params_raw):\n",
        "      # Exist or not\n",
        "      if layer_params_raw[0] < 0.5:\n",
        "        exist = False\n",
        "      else:\n",
        "        exist = True\n",
        "      # Weight function\n",
        "      tot_weight_func = 3\n",
        "      if layer_params_raw[1] < 0.6: # or ... (1/tot_weight_func):  \n",
        "        weight_func_sel = 'linear'\n",
        "      elif layer_params_raw[1] < 0.8: # or ... (2/tot_weight_func):\n",
        "        weight_func_sel = 'elem_scale'\n",
        "      else:\n",
        "        weight_func_sel = 'id'\n",
        "      # activation function\n",
        "      tot_node_func = 22\n",
        "      if layer_params_raw[2] < (1/tot_node_func):\n",
        "        activ_func = 'ReLU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (2/tot_node_func):\n",
        "        activ_func = 'Hardshrink'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (3/tot_node_func):\n",
        "        activ_func = 'Hardtanh'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = scale_to_range(layer_params_raw[4],0,2)\n",
        "        if param_1 > param_2:  # param_1 is min_val; param_2 is max_val\n",
        "          tmp_val = param_1\n",
        "          param_1 = param_2\n",
        "          param_2 = tmp_val\n",
        "        elif param_1 == param_2:\n",
        "          param_2 += 0.1\n",
        "      elif layer_params_raw[2] < (4/tot_node_func):\n",
        "        activ_func = 'LeakyReLU'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,1)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (5/tot_node_func):\n",
        "        activ_func = 'LogSigmoid'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (6/tot_node_func):\n",
        "        activ_func = 'PReLU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (7/tot_node_func):\n",
        "        activ_func = 'ELU'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (8/tot_node_func):\n",
        "        activ_func = 'ReLU6'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (9/tot_node_func):\n",
        "        activ_func = 'RReLU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (10/tot_node_func):\n",
        "        activ_func = 'SELU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (11/tot_node_func):\n",
        "        activ_func = 'CELU'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (12/tot_node_func):\n",
        "        activ_func = 'GELU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (13/tot_node_func):\n",
        "        activ_func = 'Sigmoid'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (14/tot_node_func):\n",
        "        activ_func = 'Softplus'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = scale_to_range(layer_params_raw[4],0,40)\n",
        "      elif layer_params_raw[2] < (15/tot_node_func):\n",
        "        activ_func = 'Softshrink'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (16/tot_node_func):\n",
        "        activ_func = 'Softsign'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (17/tot_node_func):\n",
        "        activ_func = 'Tanh'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (18/tot_node_func):\n",
        "        activ_func = 'Tanhshrink'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (19/tot_node_func):\n",
        "        activ_func = 'Threshold'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = scale_to_range(layer_params_raw[4],0,2)\n",
        "      elif layer_params_raw[2] < (20/tot_node_func):\n",
        "        activ_func = 'Softmin'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (21/tot_node_func):\n",
        "        activ_func = 'Softmax'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      else: \n",
        "        activ_func = 'None'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "\n",
        "      # number of nodes\n",
        "      num_nodes = scale_to_range(layer_params_raw[5], limits['min_layer_nodes'], limits['max_layer_nodes'])\n",
        "      num_nodes = num_nodes.astype(int)\n",
        "\n",
        "      return (exist, weight_func_sel, activ_func, param_1, param_2, num_nodes)\n",
        "\n",
        "    # Method for creating a layer\n",
        "    # layer_params_real format: (exist, weight_func_sel, activ_func, param_1, param_2, num_nodes)\n",
        "    def create_layer(self, prev_num_out, layer_params_real):\n",
        "      \n",
        "      exist, weight_func_sel, activ_func, param_1, param_2, num_nodes = layer_params_real\n",
        "      \n",
        "      # wf_param1/wf_param2 --> not elegant \n",
        "      if weight_func_sel == 'linear':\n",
        "        weight_func = nn.Linear\n",
        "        num_nodes_in = prev_num_out\n",
        "        num_nodes_out = num_nodes\n",
        "        wf_param1 = num_nodes_in\n",
        "        wf_param2 = num_nodes_out\n",
        "      elif weight_func_sel == 'id':\n",
        "        weight_func = nn.Identity\n",
        "        num_nodes_in = prev_num_out\n",
        "        num_nodes_out = prev_num_out\n",
        "        wf_param1 = num_nodes_in\n",
        "        wf_param2 = num_nodes_out\n",
        "      else:  # 'elem_scale'\n",
        "        weight_func = Elem_Scaling_1D\n",
        "        num_nodes_in = prev_num_out\n",
        "        num_nodes_out = prev_num_out\n",
        "        wf_param1 = num_nodes_in\n",
        "        wf_param2 = num_nodes_out\n",
        "        \n",
        "      if activ_func == 'ReLU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.ReLU())\n",
        "      elif activ_func == 'Hardshrink':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Hardshrink(param_1))\n",
        "      elif activ_func == 'Hardtanh':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Hardtanh(param_1, param_2))\n",
        "      elif activ_func == 'LeakyReLU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.LeakyReLU(param_1))\n",
        "      elif activ_func == 'LogSigmoid':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.LogSigmoid())\n",
        "      elif activ_func == 'PReLU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.PReLU())\n",
        "      elif activ_func == 'ELU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.ELU(param_1))\n",
        "      elif activ_func == 'ReLU6':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.ReLU6())\n",
        "      elif activ_func == 'RReLU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.RReLU())\n",
        "      elif activ_func == 'SELU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.SELU())\n",
        "      elif activ_func == 'CELU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.CELU(param_1))\n",
        "      elif activ_func == 'GELU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.ReLU())  # for some reason GELU is not present; revert later if relevant\n",
        "      elif activ_func == 'Sigmoid':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Sigmoid())\n",
        "      elif activ_func == 'Softplus':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softplus(param_1, param_2))\n",
        "      elif activ_func == 'Softshrink':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softplus(param_1))\n",
        "      elif activ_func == 'Softsign':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softsign())\n",
        "      elif activ_func == 'Tanh':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Tanh())\n",
        "      elif activ_func == 'Tanhshrink':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Tanhshrink())\n",
        "      elif activ_func == 'Threshold':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Threshold(param_1, param_2))\n",
        "      elif activ_func == 'Softmin':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softmin())\n",
        "      elif activ_func == 'Softmax':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softmax())\n",
        "      else:\n",
        "        a_layer = weight_func(wf_param1, wf_param2)\n",
        "\n",
        "      return a_layer, num_nodes_out\n",
        "\n",
        "\n",
        "# Generate a random chromosome where each param. is \\in [0,1)\n",
        "def gen_rand_chromosome(num_param):\n",
        "  chrom = np.random.rand(num_param)\n",
        "  return chrom\n",
        "\n",
        "# ==========================\n",
        "# Train and test functions\n",
        "# ==========================\n",
        "\n",
        "# Create a random mask for DropCircuit\n",
        "def make_mask(num_circuits, prob_drop):\n",
        "  rand_vals = np.random.rand(num_circuits)\n",
        "  decisions = rand_vals >= prob_drop\n",
        "  a_mask = np.ones(num_circuits)*decisions\n",
        "  # Must have at least one circuit active\n",
        "  if np.sum(a_mask) == 0:\n",
        "    rand_index = np.random.randint(num_circuits)\n",
        "    a_mask[rand_index] = 1.0\n",
        "\n",
        "  return torch.from_numpy(a_mask)\n",
        "\n",
        "def train_one_epoch(a_model, optimizer, epoch, batches):\n",
        "\n",
        "  a_model.train()\n",
        "  for batch_idx, (data, target) in enumerate(batches):\n",
        "      if args['cuda']:\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "      # Variables in Pytorch are differentiable. \n",
        "      \n",
        "      data, target = Variable(data), Variable(target)\n",
        "      #This will zero out the gradients for this batch. \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Create DropCircuit mask\n",
        "      dc_mask = make_mask(a_model.num_branches, args['dc_prob_drop'])\n",
        "      \n",
        "      output = a_model(data, dc_mask)\n",
        "\n",
        "      # Calculate the negative log likelihood loss.\n",
        "      loss = F.nll_loss(output, target)\n",
        "      #dloss/dx for every Variable \n",
        "      loss.backward()\n",
        "      # Gradient clipping\n",
        "      torch.nn.utils.clip_grad_norm_(a_model.parameters(), args['clip_level'])\n",
        "      #to do a one-step update on our parameter.\n",
        "      optimizer.step()\n",
        "      #Print out the loss periodically. \n",
        "\n",
        "  if args['verbose_train']:\n",
        "    if epoch % args['log_interval_epoch'] == 0:\n",
        "      print('Epoch: {}. Latest loss: {}.'.format(epoch, loss.data))\n",
        "\n",
        "# Compute accuracy\n",
        "# The argument (data_source) of this function can be a data_loader or a list of batches (previously extracted) \n",
        "def comp_accuracy(a_model, data_source, src_num_instances):\n",
        "    a_model.eval()\n",
        "    a_loss = 0\n",
        "    correct = 0\n",
        "    preds = torch.zeros(0)\n",
        "    first = True\n",
        "    for a_data_in, a_data_out in data_source:\n",
        "        if args['cuda']:\n",
        "            a_data_in, a_data_out = a_data_in.cuda(), a_data_out.cuda()\n",
        "        a_data_in, a_data_out = Variable(a_data_in), Variable(a_data_out)\n",
        "        output = a_model(a_data_in)\n",
        "\n",
        "        a_loss += F.nll_loss(output, a_data_out, reduction='sum').data # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if first:\n",
        "          preds = pred\n",
        "          first = False\n",
        "        else:\n",
        "          preds = torch.cat((preds, pred),0)\n",
        "\n",
        "        correct += pred.eq(a_data_out.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "    # Compute and return accuracy\n",
        "    if type(data_source) == list:  # case: list of batches\n",
        "      result = 100. * (correct.numpy()/ src_num_instances)\n",
        "    else: # case: data_loader\n",
        "      result = 100. * (correct.numpy()/ len(data_source.dataset))\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Train and test functions\n",
        "# ==========================\n",
        "# Compute the number of parameters in a chromose (depends on limits)\n",
        "def comp_num_chrom_param(limits):\n",
        "  # Num. of training parameters\n",
        "  tot_train_param = 4 # lr1, lr2, lr2 prop, momentum\n",
        "  # Num. of pre-branch parameters\n",
        "  tot_pre_branch = 1+(6*limits['max_pre_branch_layers']) # exist-or-not, layer params.\n",
        "  # Num. of branch parameters\n",
        "  tot_branch = (1+(6*limits['max_branch_layers']))*limits['max_branches']\n",
        "  # Num. of post-branch parameters\n",
        "  tot_post_branch = 1+(6*limits['max_post_branch_layers'])\n",
        "\n",
        "  return tot_train_param+tot_pre_branch+tot_branch+tot_post_branch\n",
        "\n",
        "# Interpret learning rate and momentum parameters\n",
        "def interp_lrm(params):\n",
        "  lr1 = scale_to_range(params[0], limits['lr1_min'], limits['lr1_,max'])\n",
        "  lr2 = scale_to_range(params[1], limits['lr2_min'], limits['lr2_,max'])\n",
        "  if lr1 < lr2:\n",
        "    temp = lr1\n",
        "    lr1 = lr2\n",
        "    lr2 = temp\n",
        "  lr2_epoch = (np.round(params[2]*args['num_epochs_search'])).astype(int)\n",
        "  a_decr = (lr1-lr2)/lr2_epoch\n",
        "  a_momentum = scale_to_range(params[3], limits['moment_min'], limits['moment_max'])\n",
        "  return lr1, lr2, lr2_epoch, a_momentum, a_decr\n",
        "\n",
        "# Chromosome evaluation (more efficient than do_training)\n",
        "# Function to train a specific model\n",
        "# Early stopping, or returning best validation model, is not implemented \n",
        "def do_eval_chrom(a_model, train_params, num_epochs):\n",
        "  global counter\n",
        "  counter += 1\n",
        "  # Extract basic information\n",
        "  lr1, lr2, lr2_epoch, a_momentum, lr_decr = train_params  \n",
        "  args['lr'] = lr1\n",
        "  args['momentum'] = a_momentum\n",
        "  \n",
        "  optimizer = optim.SGD(a_model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "  valid_accurs = []\n",
        "  train_accurs = []\n",
        "  train_start_time = time.time()\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    if args['verbose_train']:\n",
        "      print('Epoch {} learning rate: {}.'.format(epoch, args['lr']))\n",
        "\n",
        "    train_one_epoch(a_model, optimizer, epoch, train_batches)\n",
        "    \n",
        "    a_train_accur = comp_accuracy(a_model, train_batches, num_train_instances)\n",
        "    if args['verbose_train']:\n",
        "      print('Training accuracy: {}%.'.format(a_train_accur))\n",
        "    train_accurs.append(a_train_accur)\n",
        "    \n",
        "    a_valid_accur = comp_accuracy(a_model, valid_batches, num_valid_instances)\n",
        "    if args['verbose_train']:\n",
        "      print('Validation accuracy: {}%.'.format(a_valid_accur))\n",
        "    valid_accurs.append(a_valid_accur)\n",
        "\n",
        "    # Decrement learning rate\n",
        "    if epoch < lr2_epoch:\n",
        "        args['lr'] -= lr_decr\n",
        "        for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = args['lr']\n",
        "\n",
        "  train_elapsed_time = time.time() - train_start_time\n",
        "  print('The training process took {} seconds.'.format(train_elapsed_time))\n",
        "\n",
        "  return a_model, valid_accurs, train_accurs\n",
        "\n",
        "# Function to train a specific model\n",
        "# Early stopping, or returning best validation model, is not implemented \n",
        "def do_training(a_model, train_params, num_epochs):\n",
        "\n",
        "  lr1, lr2, lr2_epoch, a_momentum, lr_decr = train_params  \n",
        "  args['lr'] = lr1\n",
        "  args['momentum'] = a_momentum\n",
        "\n",
        "  optimizer = optim.SGD(a_model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "  best_valid_accur = 0\n",
        "  best_model = type(a_model)(a_model.chromosome) # get a new instance\n",
        "  valid_accurs = []\n",
        "  train_accurs = []\n",
        "  train_start_time = time.time()\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    if args['verbose_train']:\n",
        "      print('Epoch {} learning rate: {}.'.format(epoch, args['lr']))\n",
        "\n",
        "    train_one_epoch(a_model, optimizer, epoch, train_batches)\n",
        "    \n",
        "    a_train_accur = comp_accuracy(a_model, train_batches, num_train_instances)\n",
        "    if args['verbose_train']:\n",
        "      print('Training accuracy: {}%.'.format(a_train_accur))\n",
        "    train_accurs.append(a_train_accur)\n",
        "    \n",
        "    a_valid_accur = comp_accuracy(a_model, valid_batches, num_valid_instances)\n",
        "    if args['verbose_train']:\n",
        "      print('Validation accuracy: {}%.'.format(a_valid_accur))\n",
        "    valid_accurs.append(a_valid_accur)\n",
        "    if a_valid_accur > best_valid_accur:\n",
        "      best_valid_accur = a_valid_accur\n",
        "      # best_model.load_state_dict(a_model.state_dict()) # copy weights and stuff\n",
        "      best_model = copy.deepcopy(a_model)\n",
        "\n",
        "    # Decrement learning rate\n",
        "    if epoch < lr2_epoch:\n",
        "        args['lr'] -= lr_decr\n",
        "        for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = args['lr']\n",
        "\n",
        "  train_elapsed_time = time.time() - train_start_time\n",
        "  print('The training process took {} seconds.'.format(train_elapsed_time))\n",
        "\n",
        "  if args['cuda']:\n",
        "      best_model.cuda()\n",
        "\n",
        "  return a_model, best_model, valid_accurs, train_accurs\n",
        "\n",
        "# ================================\n",
        "# Architectureal search\n",
        "# ================================\n",
        "\n",
        "# Artificial Intelligence Methods students should focus on the code below.\n",
        "# You should keep the neural network code unchaged. This is crucial for \n",
        "# comparison purposes. In other words, focus only on modifying the architecture\n",
        "# search code below.\n",
        "\n",
        "# --- Architectural search parameters\n",
        "meta = {}\n",
        "meta['max_rs_iter'] = 10 # 10  # initial random search\n",
        "meta['max_shc_iter'] = 15 # 40 # 20 # 40  # stochastic hill climbing iterations\n",
        "meta['num_neighbours'] = 6 # 16 \n",
        "meta['neighbour_range'] = 0.6  # 0.2  # mutation rate for stochastic hill-climbing\n",
        "meta['num_differential_sol'] = 6 # 8 # number of differential evolution solutions\n",
        "meta['diff_lr'] = 0.5 # 0.1 # learning rate for differential search\n",
        "\n",
        "meta_rs_valids = []\n",
        "best_model = None\n",
        "best_chromosome = None\n",
        "best_model_accur = 0\n",
        "\n",
        "# Prepare model\n",
        "def prepare_model(a_rand_chrom):\n",
        "  # Initialize chromosome and model\n",
        "  model = GBNDM(a_rand_chrom)\n",
        "  \n",
        "  if args['cuda']:\n",
        "      model.cuda()\n",
        "\n",
        "  # Interpret learning rates and momentum\n",
        "  lr1, lr2, lr2_epoch, a_momentum, lr_decr = interp_lrm(a_rand_chrom[0:4])\n",
        "  train_params = (lr1, lr2, lr2_epoch, a_momentum, lr_decr)\n",
        "  args['momentum'] = a_momentum\n",
        "  if args['verbose_train']:\n",
        "    print('lr1: {}'.format(lr1))\n",
        "    print('lr2: {}'.format(lr2))\n",
        "    print('lr2_epoch: {}'.format(lr2_epoch))\n",
        "    print('a_momentum: {}'.format(a_momentum))\n",
        "    print('a_decr: {}'.format(a_decr))\n",
        "\n",
        "  return model, train_params\n",
        "\n",
        "# Function for creating one neighbour\n",
        "def create_a_neighbour(a_chromosome, neighbour_range, num_chrom_params):\n",
        "  # Create mutation vector\n",
        "  mutat_vec = (np.random.rand(num_chrom_params)*neighbour_range)-(neighbour_range/2)\n",
        "  # Add mutation vector\n",
        "  new_chromosome = a_chromosome + mutat_vec\n",
        "  # Clip\n",
        "  np.clip(new_chromosome, 0, 0.99999999999, out=new_chromosome)\n",
        "\n",
        "  return new_chromosome\n",
        "\n",
        "# Evaluate a list of chromosomes\n",
        "def eval_chromosomes(list_chromosomes,num_chrom_params):\n",
        "  \n",
        "  # best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "  \n",
        "  best_model_accur = 0\n",
        "  best_chromosome = None\n",
        "  best_model = None\n",
        "  best_train_params = None\n",
        "\n",
        "\n",
        "  num_chrom = len(list_chromosomes)\n",
        "  mat_chrom_acur = np.zeros((num_chrom, 1+num_chrom_params))\n",
        "  neighb_valid_accurs = []\n",
        "\n",
        "  for ci, a_chrom in enumerate(list_chromosomes):\n",
        "    if args['verbose_meta']:\n",
        "      print('Chromosome {} ...'.format(ci))\n",
        "    # --- Actual training\n",
        "    model, train_params = prepare_model(a_chrom)\n",
        "    model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])\n",
        "    best_valid_accur = max(valid_accurs)\n",
        "    # Store chromosome and accuracy\n",
        "    mat_chrom_acur[ci,0] = best_valid_accur\n",
        "    mat_chrom_acur[ci,1:] = a_chrom\n",
        "    print('Best validation accuracy: {}%.'.format(best_valid_accur))\n",
        "    neighb_valid_accurs.append(best_valid_accur)\n",
        "    if best_valid_accur >= best_model_accur:\n",
        "      best_model_accur = best_valid_accur\n",
        "      best_chromosome = a_chrom\n",
        "      best_model = model\n",
        "      best_train_params = train_params\n",
        "      if args['verbose_meta']:\n",
        "        print('*** Improving validation accuracy: {}.'.format(best_model_accur))\n",
        "\n",
        "  best_res = (best_model_accur, best_chromosome, best_model, best_train_params)\n",
        "  return best_res, mat_chrom_acur\n",
        "      \n",
        "    \n",
        "# Function for creating a list of neighbours\n",
        "def create_neighbours(a_chromosome, meta, num_chrom_params):\n",
        "  neighbours = []\n",
        "  \n",
        "  # Scan through number of neighbours\n",
        "  for ni in range(meta['num_neighbours']):\n",
        "    # Create first neighbour\n",
        "    next_neighb = create_a_neighbour(a_chromosome, meta['neighbour_range'], num_chrom_params)\n",
        "    # Append neighbour\n",
        "    neighbours.append(next_neighb)\n",
        "    a_chromosome = next_neighb\n",
        "\n",
        "  return neighbours\n",
        "\n",
        "# Simple differential search\n",
        "def do_diff_chrom_v1(mat_chrom_accur, num_new_sol):\n",
        "  \n",
        "  # Initialize new chromosomes\n",
        "  num_chrom = mat_chrom_accur.shape[0]\n",
        "  if num_new_sol >= num_chrom:\n",
        "    num_new_sol = num_chrom-1\n",
        "  new_chromosomes = []\n",
        "\n",
        "  # Sort the array of chromosomes based on the first column (contains accur.)\n",
        "  mat_chrom_accur = mat_chrom_accur[(-mat_chrom_accur[:,0]).argsort()]\n",
        "  # Extract first/best chromosome\n",
        "  best_chrom = mat_chrom_accur[0,1:]\n",
        "  # Scan through new solutions\n",
        "  for si in range(num_new_sol):  \n",
        "    # Extract next best chrom\n",
        "    # next_best_chrom = mat_chrom_accur[1+si,1:]\n",
        "    next_best_chrom = mat_chrom_accur[si,1:]\n",
        "    # Extract worst chrom\n",
        "    worst_chrom = mat_chrom_accur[num_new_sol-1, 1:]\n",
        "    # Compute differential\n",
        "    avg_diff = (best_chrom - worst_chrom)/ num_new_sol\n",
        "    # Add differential whilst applying a learning rate\n",
        "    a_new_chrom = best_chrom + (meta['diff_lr']*avg_diff)\n",
        "    np.clip(a_new_chrom, 0, 0.99999999999, out=a_new_chrom)\n",
        "    # Store new solution \n",
        "    new_chromosomes.append(a_new_chrom)\n",
        "\n",
        "  return new_chromosomes\n",
        "\n",
        "def final_test(a_model):\n",
        "    a_model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_preds = torch.zeros(0)\n",
        "    first = True\n",
        "    for test_data_in, test_data_out in test_loader: \n",
        "\n",
        "        if args['cuda']:\n",
        "            test_data_in, test_data_out = test_data_in.cuda(), test_data_out.cuda()\n",
        "                \n",
        "        test_data_in, test_data_out = Variable(test_data_in), Variable(test_data_out)\n",
        "        output = a_model(test_data_in)\n",
        "        test_loss += F.nll_loss(output, test_data_out, reduction='sum').data # sum up batch loss\n",
        "\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if first:\n",
        "          test_preds = pred\n",
        "          first = False\n",
        "        else:\n",
        "          test_preds = torch.cat((test_preds, pred),0)\n",
        "\n",
        "        correct += pred.eq(test_data_out.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "    # Print test accuracy\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy (at final epoch): {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "accs = []\n",
        "counter = 0\n",
        "\n",
        "for i in range(5):\n",
        "  print(\"----------------------------------------------------------\")\n",
        "  print(f\"Round: {i}\")\n",
        "  print(\"----------------------------------------------------------\")\n",
        "  best_model_accur = 0\n",
        "  best_chromosome = None\n",
        "  best_model = None\n",
        "  best_train_params = None\n",
        "  # Start with a small random search\n",
        "  print('Initital random search ...')\n",
        "  for rsi in range(meta['max_rs_iter']):\n",
        "\n",
        "    if args['verbose_meta']:\n",
        "      print('Search iteration {}.'.format(rsi+1))\n",
        "\n",
        "    num_chrom_param = comp_num_chrom_param(limits)\n",
        "    \n",
        "    a_rand_chrom = gen_rand_chromosome(num_chrom_param)\n",
        "\n",
        "    # --- Actual training\n",
        "    model, train_params = prepare_model(a_rand_chrom)\n",
        "    model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])\n",
        "    best_valid_accur = max(valid_accurs)\n",
        "    print('Best validation accuracy: {}%.'.format(best_valid_accur))\n",
        "\n",
        "    # Store best model\n",
        "    if best_valid_accur > best_model_accur:\n",
        "      best_model_accur = best_valid_accur\n",
        "      best_model = model\n",
        "      best_train_params = train_params\n",
        "      best_chromosome = a_rand_chrom\n",
        "\n",
        "    meta_rs_valids.append(best_valid_accur)\n",
        "\n",
        "  print('*****************************************************')\n",
        "  print('Best accuracy after initial random search: {}'.format(best_model_accur))\n",
        "  print('*****************************************************')\n",
        "\n",
        "  if args['verbose_meta']:\n",
        "    print('Best validation errors:')\n",
        "    print(meta_rs_valids)\n",
        "\n",
        "  # # --- Stochastic hill climbing\n",
        "\n",
        "  num_chrom_params = best_chromosome.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Architectural search iterations\n",
        "  best_res = (best_model_accur, best_chromosome, best_model, best_train_params)\n",
        "\n",
        "  meta_start_time = time.time()\n",
        "  for shci in range(meta['max_shc_iter']):\n",
        "\n",
        "    best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "\n",
        "    if best_model_accur < 15 and shci == 12:\n",
        "        best_model_accur = 0\n",
        "        best_chromosome = None\n",
        "        best_model = None\n",
        "        best_train_params = None\n",
        "        # Start with a small random search\n",
        "        print('Initital random search ...')\n",
        "        for rsi in range(12):\n",
        "\n",
        "          if args['verbose_meta']:\n",
        "            print('Search iteration {}.'.format(rsi+1))\n",
        "\n",
        "          num_chrom_param = comp_num_chrom_param(limits)\n",
        "          \n",
        "          a_rand_chrom = gen_rand_chromosome(num_chrom_param)\n",
        "\n",
        "          # --- Actual training\n",
        "          model, train_params = prepare_model(a_rand_chrom)\n",
        "          model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])\n",
        "          best_valid_accur = max(valid_accurs)\n",
        "          print('Best validation accuracy: {}%.'.format(best_valid_accur))\n",
        "\n",
        "          # Store best model\n",
        "          if best_valid_accur > best_model_accur:\n",
        "            best_model_accur = best_valid_accur\n",
        "            best_model = model\n",
        "            best_train_params = train_params\n",
        "            best_chromosome = a_rand_chrom\n",
        "        \n",
        "        # print for good measure\n",
        "        print('-------------------------------------------------------------------------------------')\n",
        "        print('REINITIALIZED')\n",
        "        print('-------------------------------------------------------------------------------------')\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "    if args['verbose_meta']:\n",
        "      print('======================================')\n",
        "      print('Stochastic hill-climbing iteration {}.'.format(shci))\n",
        "      print('======================================')\n",
        "      print('Best accuracy so far: {}'.format(best_model_accur))\n",
        "      print('======================================')\n",
        "    # --- Create a set of stochastic neighbours from the current best model\n",
        "    chrom_neighbors = create_neighbours(best_chromosome, meta, num_chrom_params)\n",
        "    # Test validation accuracies of neighbours\n",
        "    new_res, mat_chrom_acur = eval_chromosomes(chrom_neighbors,num_chrom_params)\n",
        "    if new_res[0] > best_res[0]:\n",
        "      best_res = new_res\n",
        "    print('***********************************************')\n",
        "    print('Best accuracy after random mutation: {}'.format(new_res[0]))\n",
        "    print('***********************************************')\n",
        "    \n",
        "    # --- Simple differential search\n",
        "    print('***** Differential Search *********************')\n",
        "    diff_chromosomes = do_diff_chrom_v1(mat_chrom_acur, meta['num_differential_sol'])\n",
        "    new_res, mat_chrom_acur = eval_chromosomes(diff_chromosomes,num_chrom_params)\n",
        "    if new_res[0] > best_res[0]:\n",
        "      best_res = new_res\n",
        "    print('***********************************************')\n",
        "    print('Best accuracy after differential search: {}'.format(new_res[0]))\n",
        "    print('***********************************************')\n",
        "\n",
        "  meta_elapsed_time = time.time() - meta_start_time\n",
        "  if args['verbose_meta']:\n",
        "    print('=====================================================')\n",
        "    print('Architectural optimization total time: {}.'.format(meta_elapsed_time))\n",
        "    print('=====================================================')\n",
        "    \n",
        "\n",
        "  # ==========================\n",
        "  # Test final model / Visualize predictions\n",
        "  # ==========================\n",
        "\n",
        "\n",
        "\n",
        "  # Final visualization\n",
        "  print('======================================')\n",
        "  print('Best accuracy so far: {}'.format(best_res[0]))\n",
        "  print('======================================')\n",
        "  print('Computing the final test ...')\n",
        "  best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "  final_model, best_valid_model, valid_accurs, train_accurs = do_training(best_model, best_train_params, args['num_epochs_test'])\n",
        "  print('=====================================')\n",
        "  print('Training accuracies')\n",
        "  print('=====================================')\n",
        "  print(train_accurs)\n",
        "  print('=====================================')\n",
        "  print('Best training parameters')\n",
        "  print('=====================================')\n",
        "  print(best_train_params)\n",
        "  print('=====================================')\n",
        "  print('Best model')\n",
        "  print('=====================================')\n",
        "  print(best_valid_model)\n",
        "  print('=====================================')\n",
        "  print('Test accuracies')\n",
        "  print('=====================================')\n",
        "  print('Model with best validation accuracy: ')\n",
        "  accur_valid = final_test(best_valid_model)\n",
        "  print('Model at the end of training: ')\n",
        "  accur_final = final_test(final_model)\n",
        "\n",
        "  max_accur = int(max(accur_valid, accur_final))\n",
        "  accs.append(max_accur)\n",
        "\n",
        "  # --- Saving the best chromosome as a csv file\n",
        "  if args['save_best_chrom']:\n",
        "\n",
        "    print('Saving the best chromosome in the root directory (My Drive) ...')\n",
        "\n",
        "    from datetime import datetime\n",
        "    from numpy import asarray\n",
        "    from numpy import savetxt\n",
        "    import numpy as np\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount('/content/gdrive/')\n",
        "\n",
        "    def get_date_time_str():\n",
        "      now = datetime.now() # current date and time\n",
        "      return now.strftime(\"%d%m%Y_%H%M\")\n",
        "\n",
        "    # Create filename\n",
        "    path = '/content/gdrive/My Drive/'\n",
        "    filename = 'chrom_' + get_date_time_str() + version + str(max_accur)\n",
        "    savetxt(path+filename, best_chromosome, delimiter=',')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S265bQJTnTgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import statistics\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Mutate the best chromosome to form 50 new chromosomes\n",
        "def mutate_init_chrom(best_chromosome, init_mutation_rate):\n",
        "  new_chromosome = best_chromosome # np.array(best_chromosome)\n",
        "  # Initialise population\n",
        "  for i in range(50):\n",
        "    if random.uniform(0,1) > init_mutation_rate:\n",
        "      init_population.append(best_chromosome)\n",
        "    else:\n",
        "      # mutate 10% genes of chromosomes\n",
        "      for gene in range(37):\n",
        "        new_chromosome[gene] = random.uniform(0, 1) \n",
        "      temp_population.append(new_chromosome)\n",
        "      new_chromosome = np.array(best_chromosome)\n",
        "  \n",
        "  # Replace parent with better chromosomes  \n",
        "  best_res, mat_chrom_acur = eval_chromosomes(init_population, num_chrom_params)\n",
        "  best_res_2, mat_chrom_acur_2 = eval_chromosomes(temp_population, num_chrom_params)\n",
        "  for i, chrom in enumerate(temp_population):\n",
        "    # Accept better chromosomes into init population\n",
        "    if mat_chrom_acur_2[i, 0] > mat_chrom_acur[i, 0]:\n",
        "      init_population.append(temp_population[i])\n",
        "    else:\n",
        "      init_population.append(init_population[i])\n",
        "\n",
        "  return init_population\n",
        "  \n",
        "\n",
        "def selection_chrom(init_population, num_chrom_selected):\n",
        "  individual_sum_of_fitness = 0\n",
        "  total_fitness = 0\n",
        "  fitness_level = 0\n",
        "  total_new_fitness = 0\n",
        "  cummulative_probability = 0\n",
        "  population_fitness = []\n",
        "\n",
        "  print(\"-------SELECTION OPERATION--------\")\n",
        "  best_res, mat_chrom_acur = eval_chromosomes(init_population, num_chrom_params)\n",
        "\n",
        "  for i in range(50):\n",
        "      total_fitness += mat_chrom_acur[i, 0]\n",
        "\n",
        "  print(\"\\nModified Roulette Wheel selection\")\n",
        "  max_accur = math.ceil(max(mat_chrom_acur[0:50, 0])) + 1\n",
        "  print(max_accur)\n",
        "  print(\"Chromosome\\tFitness level\\tProbability of selection\\tCummulative probability\")\n",
        "\n",
        "  for i in range(50):\n",
        "    total_new_fitness += ((mat_chrom_acur[i,0]*0.01)/math.sqrt(1 - ((pow(mat_chrom_acur[i,0], 2)/(pow(max_accur, 2))))))\n",
        " \n",
        "  for i in range(50):\n",
        "    probability = ((mat_chrom_acur[i,0]*0.01)/math.sqrt(1 - ((pow(mat_chrom_acur[i,0], 2)/(pow(max_accur, 2))))))/total_new_fitness\n",
        "    cummulative_probability += probability\n",
        "    population_fitness.append(cummulative_probability)\n",
        "    print(\"{}\\t\\t{:.2f}\\t\\t\\t{:.4f}\\t\\t\\t{:.4f}\".format(i + 1, mat_chrom_acur[i,0], probability, cummulative_probability))\n",
        "\n",
        "  # Choosing the top 10\n",
        "  print(\"Selecting top poulation\\n\")\n",
        "  for selection in range(num_chrom_selected):\n",
        "    random_prob = random.uniform(0,1)\n",
        "    print(\"Random probability:\\t\", end=\"\")\n",
        "    print(\"{:.4f}\".format(random_prob))\n",
        "    for i in range(50):\n",
        "      if random_prob < population_fitness[i]:\n",
        "        print(\"Population fitness:\\t \", end=\"\")\n",
        "        print(\"{:.4f}\".format(population_fitness[i]))\n",
        "        print(\"Selected chromosome: {}\".format(i+1))\n",
        "        top_population.append(init_population[i])\n",
        "        break\n",
        "\n",
        "  return top_population\n",
        "\n",
        "def crossover_chrom(top_population, crossover_probability):\n",
        "  # Crossover operation\n",
        "  print(\"------CROSSOVER OPERATION--------\")\n",
        "  parents = []\n",
        "  parent1 = []\n",
        "  parent2 = []\n",
        "  parent3 = []\n",
        "  parent4 = []\n",
        "  parent5 = []\n",
        "  for i in range (10):\n",
        "    # Select two different parents\n",
        "    print(i)\n",
        "    parent1 = []\n",
        "    parents = random.sample(range(1,10), 5)\n",
        "    print(\"Parents: \", end=\"\")\n",
        "    print(parents)\n",
        "    parent1 = top_population[parents[0]]\n",
        "    parent2 = top_population[parents[1]]\n",
        "    parent3 = top_population[parents[2]]\n",
        "    parent4 = top_population[parents[3]]\n",
        "    parent5 = top_population[parents[4]]\n",
        "    \n",
        "    if random.uniform(0,1) < crossover_probability:\n",
        "      # Extract genes from multiple parents\n",
        "      parent1[76:150] = parent2[76:150]\n",
        "      parent1[151:225] = parent3[151:225]\n",
        "      parent1[226:300] = parent4[226:300]\n",
        "      parent1[301:375] = parent5[301:375]\n",
        "\n",
        "    crossover_population.append(parent1)\n",
        "  return crossover_population\n",
        "\n",
        "def mutation_chrom(crossover_population, mutation_probability):\n",
        "  mutation_rate = 0.1\n",
        "  new_mutated_chromosome = []\n",
        "  test_population = []\n",
        "  current = []\n",
        "  \n",
        "  for i in range(10):\n",
        "    new_mutated_chromosome = []\n",
        "    new_mutated_chromosome = crossover_population[i]\n",
        "    test_population = []\n",
        "    if random.uniform(0,1) < mutation_probability:\n",
        "      mutation_population.append(new_mutated_chromosome)\n",
        "    else:\n",
        "      # mutate 10% genes of chromosomes\n",
        "      current = new_mutated_chromosome\n",
        "      test_population.append(current)\n",
        "      for gene in range(37):\n",
        "        new_mutated_chromosome[gene] = random.uniform(0, 1)  \n",
        "      test_population.append(new_mutated_chromosome)\n",
        "      best_res, mat_chrom_acur_3 = eval_chromosomes(test_population, num_chrom_params)\n",
        "      if mat_chrom_acur_3[1, 0] >  mat_chrom_acur_3[0, 0]:\n",
        "        mutation_population.append(new_mutated_chromosome)\n",
        "      else:\n",
        "        mutation_population.append(current)\n",
        "\n",
        "  return mutation_population\n",
        "\n",
        "def do_diff_chrom_v2(mat_chrom_acur, mutation_population):\n",
        "  print(\"---LOCAL SEARCH---\")\n",
        "  offspring = []\n",
        "  neighbouring_offspring = []\n",
        "  c_population = []\n",
        "  new_c = []\n",
        "  sum_fitness = 0\n",
        "\n",
        "  best_res, mat_chrom_acur = eval_chromosomes(mutation_population, num_chrom_params)\n",
        "  for i in range(10):\n",
        "    print(\"{}\\t\\t{:.2f}\".format(i + 1, mat_chrom_acur[i,0]))\n",
        "    neighbouring_offspring.append(mutation_population[i])\n",
        "    sum_fitness += mat_chrom_acur[i, 0]\n",
        "  max_fitness = max(mat_chrom_acur[0:10, 0])\n",
        "  avg_fitness = sum_fitness/10\n",
        "  l_rate = 1/math.ceil(max_fitness - avg_fitness)\n",
        "\n",
        "  for j in range(10):\n",
        "    c_population = []\n",
        "    distance = (l_rate*random.uniform(0,1))\n",
        "    new_c = neighbouring_offspring[j] + distance\n",
        "    c_population.append(mutation_population[j])\n",
        "    c_population.append(new_c)\n",
        "    \n",
        "    best_res, mat_chrom_acur_3 = eval_chromosomes(c_population, num_chrom_params)\n",
        "    if mat_chrom_acur_3[1, 0] >  mat_chrom_acur_3[0, 0]:\n",
        "      diff = mat_chrom_acur_3[1,0] - mat_chrom_acur_3[0,0]\n",
        "      # Replace if the difference is signifant\n",
        "      if diff > 0.5:\n",
        "        mutation_population[j] = new_c\n",
        "\n",
        "  return mutation_population\n",
        "\n",
        "acc_generation = []\n",
        "for generation in range(10):\n",
        "  population = 30\n",
        "  init_population = []\n",
        "  temp_population = []\n",
        "  top_population = []\n",
        "  crossover_population = []\n",
        "  mutation_population = []\n",
        "  differential_chromosomes = []\n",
        "  init_mutation_rate = 0.2\n",
        "  num_chrom_selected = 10\n",
        "  crossover_probability = 0.6\n",
        "  mutation_probability = 0.1\n",
        "  print(\"GENERATION: {}\".format(generation+1))\n",
        "\n",
        "  # Initialise population\n",
        "  mutate_init_chrom(best_chromosome, init_mutation_rate)\n",
        "  # Selection operation\n",
        "  selection_chrom(init_population, num_chrom_selected)\n",
        "  # Crossover operation\n",
        "  crossover_chrom(top_population, crossover_probability)\n",
        "  # Mutation operation\n",
        "  mutation_chrom(crossover_population, mutation_probability)\n",
        "  # Local search\n",
        "  differential_chromosomes = do_diff_chrom_v2(mat_chrom_acur, mutation_population)\n",
        "  best_res, mat_chrom_acur_2 = eval_chromosomes(differential_chromosomes,num_chrom_params)\n",
        "  best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "\n",
        "  total_fitness = 0\n",
        "  average_fitness = 0\n",
        "  for i in range(10):\n",
        "      total_fitness += mat_chrom_acur_2[i, 0]\n",
        "  average_fitness = total_fitness/10\n",
        "  max_acc = max(mat_chrom_acur_2[0:10, 0])\n",
        "  print(\"BEST FITNESS: \", end=\"\")\n",
        "  print(max_acc)\n",
        "  print(\"AVERAGE FITNESS: \", end=\"\")\n",
        "  print(average_fitness)\n",
        "\n",
        "\n",
        "  print('Best accuracy so far: {}'.format(best_res[0]))\n",
        "  print('======================================')\n",
        "  print('Computing the final test ...')\n",
        "  best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "  final_model, best_valid_model, valid_accurs, train_accurs = do_training(best_model, best_train_params, args['num_epochs_test'])\n",
        "  print('=====================================')\n",
        "  print('Training accuracies')\n",
        "  print('=====================================')\n",
        "  print(train_accurs)\n",
        "  print('=====================================')\n",
        "  print('Best training parameters')\n",
        "  print('=====================================')\n",
        "  print(best_train_params)\n",
        "  print('=====================================')\n",
        "  print('Best model')\n",
        "  print('=====================================')\n",
        "  print(best_valid_model)\n",
        "  print('=====================================')\n",
        "  print('Test accuracies')\n",
        "  print('=====================================')\n",
        "  print('Model with best validation accuracy: ')\n",
        "  accur_valid = final_test(best_valid_model)\n",
        "  print('Model at the end of training: ')\n",
        "  accur_final = final_test(final_model)\n",
        "\n",
        "  max_accur = int(max(accur_valid, accur_final))\n",
        "  acc_generation.append(max_accur)\n",
        "\n",
        "  # Average fitness condition met\n",
        "  if max_accur > 77:\n",
        "    print(\"---Validation test ACCEPTED---\")\n",
        "    break\n",
        "  else:\n",
        "  # Continue to next generation\n",
        "    print(\"---Validation test REJECTED. Proceeding to next generation...\")\n",
        "\n",
        "print(\"Best results for each generation:\")\n",
        "print(acc_generation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-mGoTbboSY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "76df224d-9eb3-445b-efdc-adfd511b06e2"
      },
      "source": [
        "# code from cw1\n",
        "\n",
        "import copy\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "initial_accs = [72, 73, 46, 43, 70, 17, 70, 66, 72, 18]\n",
        "tmp_accs2 = [75, 75, 76, 76, 74, 75, 74, 71, 72, 74] \n",
        "\n",
        "print(\"Original: \")\n",
        "# for i, f in enumerate(tmp_accs):\n",
        "#     print(\"Round {}:\\t{:.2f}\".format(i + 1, f))\n",
        "print(\"\\nMean Acc:\\t\", end=\"\")\n",
        "print(\"{:.2f}\".format(sum(initial_accs) / len(initial_accs)))\n",
        "print(\"Median Acc:\\t\", end=\"\")\n",
        "tmp_accs_sort = copy.deepcopy(initial_accs)\n",
        "tmp_accs_sort.sort()\n",
        "print(\"{:.2f}\".format(tmp_accs_sort[len(tmp_accs_sort) // 2]))\n",
        "print(\"1st Quart Acc:\\t\", end=\"\")\n",
        "print(\"{:.2f}\".format(tmp_accs_sort[len(tmp_accs_sort) // 4]))\n",
        "print(\"3rd Quart Acc:\\t\", end=\"\")\n",
        "print(\"{:.2f}\".format(tmp_accs_sort[len(tmp_accs_sort) // 4 * 3]))\n",
        "print(\"Inter Quart:\\t{}\".format(tmp_accs_sort[len(tmp_accs_sort) // 4 * 3] - tmp_accs_sort[len(tmp_accs_sort) // 4]))\n",
        "print(\"Standard dev:\\t{}\".format(statistics.stdev(initial_accs)))\n",
        " \n",
        "print(\"\\n\\nMy Results: \")\n",
        "# for i, f in enumerate(tmp_accs2):\n",
        "#     print(\"Round {}:\\t{:.2f}\".format(i + 1, f))\n",
        "print(\"\\nMean Acc:\\t\", end=\"\")\n",
        "print(\"{:.2f}\".format(sum(tmp_accs2) / len(tmp_accs2)))\n",
        "print(\"Median Acc:\\t\", end=\"\")\n",
        "tmp_accs_sort_2 = copy.deepcopy(tmp_accs2)\n",
        "tmp_accs_sort_2.sort()\n",
        "print(\"{:.2f}\".format(tmp_accs_sort_2[len(tmp_accs_sort_2) // 2]))\n",
        "print(\"1st Quart Acc:\\t\", end=\"\")\n",
        "print(\"{:.2f}\".format(tmp_accs_sort_2[len(tmp_accs_sort_2) // 4]))\n",
        "print(\"3rd Quart Acc:\\t\", end=\"\")\n",
        "print(\"{:.2f}\".format(tmp_accs_sort_2[len(tmp_accs_sort_2) // 4 * 3]))\n",
        "print(\"Inter Quart:\\t{}\".format(tmp_accs_sort_2[len(tmp_accs_sort_2) // 4 * 3] - tmp_accs_sort_2[len(tmp_accs_sort_2) // 4]))\n",
        "print(\"Standard dev:\\t{}\".format(statistics.stdev(tmp_accs2)))\n",
        " \n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Boxplot of hill climbing vs genetic algorithm\")\n",
        "ax = plt.subplot(1, 2, 1)\n",
        "ax.boxplot(initial_accs)\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel(\"Hill Climbing\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        " \n",
        "ax = plt.subplot(1, 2, 2)\n",
        "ax.boxplot(tmp_accs2)\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel(\"Genetic Algorithm\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: \n",
            "\n",
            "Mean Acc:\t54.70\n",
            "Median Acc:\t70.00\n",
            "1st Quart Acc:\t43.00\n",
            "3rd Quart Acc:\t70.00\n",
            "Inter Quart:\t27\n",
            "Standard dev:\t22.385759362197707\n",
            "\n",
            "\n",
            "My Results: \n",
            "\n",
            "Mean Acc:\t74.20\n",
            "Median Acc:\t75.00\n",
            "1st Quart Acc:\t74.00\n",
            "3rd Quart Acc:\t75.00\n",
            "Inter Quart:\t1\n",
            "Standard dev:\t1.6193277068654826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}